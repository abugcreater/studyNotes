# 22.  Kubernetes的资源模型与资源管理

Pod的资源控制主要是通过配置CPU和内存,如下所示:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: frontend
spec:
  containers:
  - name: db
    image: mysql
    env:
    - name: MYSQL_ROOT_PASSWORD
      value: "password"
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
  - name: wp
    image: wordpress
    resources:  # 资源配置,请求资源为64M, 1/4的CPU,最大是 127M, 0.5个CPU
      requests:     
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
```

> 所谓 500m，指的就是 500 millicpu，也就是 0.5 个 CPU 的意思
>
> 而对于内存资源来说，它的单位自然就是 bytes。Kubernetes 支持你使用 Ei、Pi、Ti、Gi、Mi、Ki（或者 E、P、T、G、M、K）的方式来作为 bytes 的值。

CPU资源被称为可压缩资源(compressible resources),当可压缩不足时,Pod只会饥饿,不会退出.

内存则被称为不可压缩资源(uncompressible resources),不可压缩资源不足时,Pod就会OOM被内核杀掉.

一个Pod的资源配置由Pod中所有的Container的配置累加得到.

---

**Kubernetes 里 Pod 的 CPU 和内存资源，实际上还要分为 limits 和 requests 两种情况**，如下所示：

```
spec.containers[].resources.limits.cpu
spec.containers[].resources.limits.memory
spec.containers[].resources.requests.cpu
spec.containers[].resources.requests.memory
```

两者区别在于,**kube-scheduler只会按照requests进行计算,但是设置Cgroups限制时,kubelet会按照limits值进行.**

**Kubernetes 这种对 CPU 和内存资源限额的设计，实际上参考了 Borg 论文中对“动态资源边界”的定义**，既：容器化作业在提交时所设置的资源边界，并不一定是调度系统所必须严格遵守的，这是因为在实际场景中，**大多数作业使用到的资源其实远小于它所请求的资源限额**。

---

 **Kubernetes 里的 QoS 模型**

**Guaranteed类型:Pod中每个Container都设置了requests和limits且这两个值都相等**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo
  namespace: qos-example
spec:
  containers:
  - name: qos-demo-ctr
    image: nginx
    resources:
      limits:
        memory: "200Mi"
        cpu: "700m"
      requests:
        memory: "200Mi"
        cpu: "700m"
```

该对象创建后,qosClass字段会自动设置为Guaranteed.当仅设置limits是K8s会自动设置与limits相同的requests也属于Guaranteed.

**Burstable类型,当Pod不满足Guaranteed,但至少有一个Container设置了requests**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo-2
  namespace: qos-example
spec:
  containers:
  - name: qos-demo-2-ctr
    image: nginx
    resources:
      limits
        memory: "200Mi"
      requests:
        memory: "100Mi"
```

**BestEffort类型,如果一个Pod既没有设置requests,也没有设置limits**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo-3
  namespace: qos-example
spec:
  containers:
  - name: qos-demo-3-ctr
    image: nginx
```

---

当不可资源短缺是,Kubernetes 将不会再宿主机上分配Pod,并可能触发驱逐(Eviction),Eviction的默认阈值如下所示:

```
memory.available<100Mi
nodefs.available<10%
nodefs.inodesFree<5%
imagefs.available<15%
```

**Eviction 在 Kubernetes 里其实分为 Soft 和 Hard 两种模式**

- Soft模式允许设置一段"优雅时间",在阈值到达后,经过这段时间才开始驱逐
- Hard模式会在阈值达到后立刻开始

> Kubernetes 计算 Eviction 阈值的数据来源，主要依赖于从 Cgroups 读取到的值，以及使用 cAdvisor 监控到的数据。

kubelet 挑选Pod是根据Pod的QoS类别:

- 首先是BestEffort
- 然后是Burstable ,且发生饥饿的资源使用量已经超过了requests的
- 最后是Guaranteed ,当这种类型的Pod资源使用已经超过limits限制,或者宿主机本身正处于 Memory Pressure 状态时，Guaranteed 的 Pod 才可能被选中进行 Eviction 操作

---

**Kubernetes 里一个有用的特性：cpuset 的设置**

 可以通过设置 cpuset 把容器绑定到某个 CPU 的核上，而不是像 cpushare 那样共享 CPU 的计算能力。

**cpuset 方式，是生产环境里部署在线应用类型的 Pod 时，非常常用的一种方式。**

配置要求是Pod是Guaranteed 的QoS类型,设置了CPU的参数,如下所示:

```yaml
spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
        cpu: "2"
      requests:
        memory: "200Mi"
        cpu: "2"
```

Kubelet会自动分配2个CPU核来绑定该Pod

















































































